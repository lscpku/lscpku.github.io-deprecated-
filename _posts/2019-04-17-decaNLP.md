---
title: DecaNLP
categories: [NLP]
tags: [model]
date: 2019-04-17
author: lscpku
---

{% include MathJaxHead.html %}

# {{page.title}}

{{page.date | date: '%Y-%m-%d'}} by {{page.author}}

{% for tag in page.tags %}`{{ tag }}` {% endfor %}

---

# DecaNLP
Richard Socher
[The Natural Language Decathlon: Multitask Learning as QA](https://arxiv.org/pdf/1806.08730.pdf) by Bryan McCann et al.

## Motivation
### What's next for NLP & AI
- ML with feature engineering
- DL for feature learning
- Deep architecture engineering for single tasks


### The limits of Single-taks learning
- For more general AI, we need continuous learning in a single model
- keep restarting cannot capture complexity of NL

### pre-training and sharing knowledge is great
- CV: ImageNet+CNN.
  The blocking task: classification
- NLP: Word2Vec, GloVe,  ELMo, BERT.
  No single blocking task

### Why not pre-train the entire model
- NLP requires many types of reasoning: logical, linguistic, emotional, etc.
- NLP has been divided into intermediate and separate tasks to make progress
- Language requires supervision

### Why a unified multi-task model for NLP
- Multi-task learning is a blocker for general NLP systems
- Unified models can decide how to transfer knowledge(zero-shot, transfer learning, weight sharing, domain adapatation)
	- adapt to new tasks
	- production faster
	- potentianally move towards continual learning

## Solution
### How to express many NLP tasks in the same framework?
- Seq tagging: NER, aspect specific sentiment
- Text classification: Dialogue state tracking, sentiment classification 
- seq2seq: MT, summarization, QA

### 3 equivalent Supertasks of NLP
- Language Modeling
- QA
- Dialogue

### The natural language decathlon (decaNLP)
- Transform all tasks into QA
- Question -- Context -- Answer
- For example: <br>
Question: Is this sentence positive or negative <br>
Context: the sentence <br>
Answer: positive <br>
(bonus -- answer comes from question. may achieve zero-shot learning.)

### Multitask Learning as QA
- QA, Semantic Role Labeling, MT, Relation Extraction, Summarization, Dialogue, NLI, Semantic Parsing, Sentiment Classification, Commonsense Reasoning
- Meta-Supervised learning: from (x,y) to (x,t,y) where t is the task
- q as a natural description of task t, y is the answer to q given necessary context x

### Model design
- No task-specific module or param
- Must be able to adjust internally to perform disparate tasks
- Should leave open the possibility of zero-shot inference for unseen tasks

- Start with a context
- Ask a question
- Generate the answer one word at a time by pointing to context, pointing to question, or choosing a word from external voc ([pointer network](to be added))
- Pointer Switch chooses between those three options for each output word

### Multitask QA Network (MQAN)
1. Input Question, Context
2. GloVe/CoVe/BERT Embeddings: fixed. Otherwise will not generalize well.
3. Linear layer
4. Shared Deep BiLSTM with skip connextions. Shared set of weights between Context and Question.
5. Co-attention layer: attention summations from one sequence to the other and back again with skip connections.
6. Separate BiLSTMs to reduce dimensionality, two transform layers, another BiLSTM. 
	- Tried use transformer in all places but is hard to optimize. Transformers which work well for different tasks may have different layers. Not robust enough.
7. Auto-regressive decoder uses fixed GloVe and character n-gram embeddings, two transformer layers and an LSTM layer that attend to outputs of the last three layers of the encoder.
8. The LSTM decoder state is used to compute attention distributions over the context and question which are used as pointers
9. Compute weighted sum of distribution of output words.

- Output is only one word.


---

[返回目录](/table_of_posts.html)
