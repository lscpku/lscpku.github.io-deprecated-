---
title: NLP tasks
categories: [nlp]
tags: [nlp]
date: 2019-04-17
author: lscpku
---

{% include MathJaxHead.html %}

# {{page.title}}

{{page.date | date: '%Y-%m-%d'}} by {{page.author}}

{% for tag in page.tags %}`{{ tag }}` {% endfor %}

---

## Qustion answering

### 1. SQuAD
- The Stanford Question Answering Dataset contains 100K+ crowd sourced question-answer pairs where the answer is a span in a given Wikipedia paragraph.

## GLUE

### MNLI
- Multi-Genre Natural Language Inference
- entailment classification
- Given a pair of sentences, the goal is to predict whether the second is an *entailment*, *contradiction*, or *neutral* with respect to the first one.

### QQP
- Quora Question Pairs
- binary classification task
- determine if two questions asked on Quora are semantically equivalent

### QNLI
- Question Natural Language Inference
- a binary classification task converted from SQuAD
- Given a pair (question, sentence), determine whether the sentence contains the correct answer for the question.

### SST-2
- The Stanford Sentiment Treebank
- a binary single-sentence classification
- consists of sentences extracted from movie reviews with their sentiment labeled by human

### SST-5
- SST with five labels

### CoLA
- The Corpus of Linguistic Acceptability
- a binary single-sentence classification
- Predict whether an English sentence is linguistically "acceptable" or not

### STS-B
- The Semantic Textual Similarity Benchmark
- sentence pairs annotated with a score from 1 to 5 denoting the semantic similarity between them

### MRPC
- Microsoft Research Paraphrase Corpus
- sentence pairs extracted from news sources
- with human annotations for whether they are semantically equivalent

### RTE
- Recognizing Textual Entailment
- binary entailment task
- similar to MNLI but with much less training data

### WNLI
- Winograd NLI
- a small natural language inference dataset
- (issues with construction according to BERT)

## 3. Named Entity Recognition
### CoNLL 2003 NER
- consists of 200k training words which have been labeled as *Person*, *Organization*, *Location*, *Miscellaneous* or *Other* (non-named entity).

### SWAG
- The Situations With Adversarial Generations
- contains 113k sentence-pair completion examples that evaluate grounded commensense inference.
- The task is to decide among four options the most plausible continuation of the given sentence.

## Textual Entailment
- the task of determining whether a "hypothesis" is true, given a premise.

### SNLI
- The Stanford Natural Language Inference
- contains about 550k hypothesis/premise pairs

## Semantic Role Labeling
- model the predicate-argument structure of a sentence.

## Coreference Resolution

---

[返回目录](/table_of_posts.html)